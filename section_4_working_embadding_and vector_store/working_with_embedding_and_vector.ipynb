{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff35b154",
   "metadata": {},
   "source": [
    "### Working with External Documents in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd38dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-community\n",
    "# !pip install unstructured\n",
    "# !pip install pdfminer\n",
    "# !pip install --upgrade unstructured[pdf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba8ae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "docs_folder = \"./docs\"\n",
    "documents = []\n",
    "\n",
    "for filename in os.listdir(docs_folder):\n",
    "    filepath = os.path.join(docs_folder, filename)\n",
    "    print(f\"Loading {filepath}...\"  )\n",
    "    loader = UnstructuredPDFLoader(filepath)\n",
    "    documents.extend(loader.load())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6688b0",
   "metadata": {},
   "source": [
    "##### Chanking text into small parts can be usefull for processing large documents. Here's how you can do it using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd75fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50)\n",
    "chanks = splitter.split_documents(documents)\n",
    "\n",
    "for i, chank in enumerate(chanks):\n",
    "    print(f\"--- Chank {i+1} ---\")\n",
    "    print(chank.page_content)\n",
    "    print(\"--------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16766af",
   "metadata": {},
   "source": [
    "### Embedding Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8445feb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "embedding = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "db = Chroma.from_documents(chanks, embedding, persist_directory=\"./qa_db\")\n",
    "\n",
    "retriever = db.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e5883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6d990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model = \"qwen2.5:7b\",\n",
    "    temperature=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55d8859",
   "metadata": {},
   "source": [
    "#### Retrival QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833ad26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever  \n",
    ")\n",
    "\n",
    "question = \"Explain system architecture of Netflix?\"\n",
    "print(qa_chain.run(question))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
